# DataThon-Fiap


Back
https://flask-service-1092765354740.us-central1.run.app/

# Proposta de SoluÃ§Ã£o

## Contexto

A criaÃ§Ã£o de um fluxo padrÃ£o de entrevistas devido Ã  grande diversidade de formatos de currÃ­culos e vagas recebidas.

## Objetivo

Desenvolver uma API capaz de automatizar o processo de anÃ¡lise de currÃ­culos e vagas, com foco em:

Resumir informaÃ§Ãµes do candidato e da vaga.

Gerar perguntas estruturadas para o entrevistador, cobrindo todos os pontos essenciais da vaga.

Padronizar o retorno da anÃ¡lise, tornando a experiÃªncia da entrevista mais eficiente e consistente.

Calcular uma taxa de compatibilidade entre o currÃ­culo e a vaga, auxiliando na triagem de candidatos.

## BenefÃ­cios

PadronizaÃ§Ã£o das entrevistas.

Agilidade na preparaÃ§Ã£o de perguntas para entrevistadores.

AvaliaÃ§Ã£o objetiva da compatibilidade entre candidatos e vagas.

# Desenvolvimento do Modelo

Este projeto utiliza tÃ©cnicas de RAG (Retrieval-Augmented Generation) e LLMs (Large Language Models) para processar dados de currÃ­culos e vagas, gerando resumos estruturados, perguntas e uma taxa de compatibilidade entre candidato e vaga.

## VisÃ£o Geral da SoluÃ§Ã£o

### Subida e PreparaÃ§Ã£o de Dados

Inicialmente, os dados fornecidos em arquivos JSON sÃ£o carregados em tabelas no GCP.

Essas tabelas servem como base de testes durante o desenvolvimento, permitindo o aperfeiÃ§oamento de prompts e inputs.

### CriaÃ§Ã£o da Tabela Tratada

ApÃ³s a subida, criamos uma tabela tratada que cruza dados de vagas e currÃ­culos.

Esta tabela Ã© utilizada diretamente nos scripts Python para gerar anÃ¡lises e resumos.

### PrÃ©-processamento de Texto

Textos de vagas e currÃ­culos passam por normalizaÃ§Ã£o, incluindo:

- RemoÃ§Ã£o de espaÃ§os desnecessÃ¡rios
- EliminaÃ§Ã£o de palavras duplicadas
- RemoÃ§Ã£o de acentos
- Filtragem de stopwords

## Testes com Diferentes Modelos LLM

Foram testadas diversas arquiteturas de LLMs, tanto pelo HuggingFace quanto pelo Ollama, incluindo modelos como Gemma, DeepSeek, LLaMA, QWEN.

Foram avaliados tempo de resposta e qualidade das respostas, buscando identificar a configuraÃ§Ã£o ideal para a soluÃ§Ã£o.

## Modelo Final

A versÃ£o final da soluÃ§Ã£o utiliza:

- Um LLM de text-generation com RAG para gerar resumos e perguntas estruturadas.
- Um LLM de similarity com Sentence-Transformers para calcular a taxa de compatibilidade entre candidato e vaga.

# STACK Utilizada

- React com Vite para Frontend
- API FLask em python para backend
- Transformers com modelos do Hugging-Face(gemma3:4b) para o modelo final

# Backend DataThon-FIAP

Este backend foi desenvolvido em Python utilizando Flask para servir rotas REST e Google BigQuery para persistÃªncia dos dados de avaliaÃ§Ã£o. A arquitetura estÃ¡ organizada para facilitar a escalabilidade e manutenÃ§Ã£o.

## Estrutura de Pastas

```
## Estrutura de Pastas

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ questions.py
â”‚   â”‚   â””â”€â”€ submit_evaluation.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ call_model.py
â”‚       â”œâ”€â”€ create_table.py
â”‚       â”œâ”€â”€ model_rag.py
â”‚       â”œâ”€â”€ preprocessing.py
â”‚       â”œâ”€â”€ questions_example.json
â”‚       â””â”€â”€ settings.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_question.py
â”‚   â””â”€â”€ test_submit_evaluation.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ deploy.ps1
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ .gitignore
```

## Como Rodar o Projeto

1. **PrÃ©-requisitos**
   - Python 3.8+
   - Conta e projeto no Google Cloud com BigQuery habilitado
   - Credencial de serviÃ§o (`gcp_key.json`)

2. **Instale as dependÃªncias:**
   ```powershell
   pip install -r requirements.txt
   ```

3. **Configure a autenticaÃ§Ã£o do Google Cloud:**
   - Coloque o arquivo `gcp_key.json` na raiz do backend.
   - Defina a variÃ¡vel de ambiente:
     ```powershell
     $env:GOOGLE_APPLICATION_CREDENTIALS="caminho\para\gcp_key.json"
     ```

4. **Execute o servidor Flask:**
   ```powershell
   python main.py
   ```
   O servidor estarÃ¡ disponÃ­vel em `http://localhost:5000`.

## Rotas DisponÃ­veis

### POST `/questions`
- **DescriÃ§Ã£o:** Recebe dados para geraÃ§Ã£o de perguntas.
- **Payload esperado:**
  ```json
  {
    "candidate": {...},
    "job": {...}
  }
  ```
- **Resposta:**
  ```json
  {
    "questions": ["Pergunta 1", "Pergunta 2", ...]
  }
  ```

### POST `/submit_evaluation`
- **DescriÃ§Ã£o:** Registra uma avaliaÃ§Ã£o no BigQuery.
- **Payload esperado:**
  ```json
  {
    "json_sent": {...},
    "json_received": {...},
    "rating": 1-5
  }
  ```
- **Resposta:**
  ```json
  {
    "message": "AvaliaÃ§Ã£o registrada com sucesso!",
    "evaluation_id": "<uuid>"
  }
  ```

## Testes

Os testes unitÃ¡rios estÃ£o na pasta `tests/` e podem ser executados com:
```powershell
pytest
```

## ObservaÃ§Ãµes
- O projeto utiliza o Google BigQuery, portanto Ã© necessÃ¡rio ter uma conta e projeto configurados.
- O arquivo `requirements.txt` lista todas as dependÃªncias necessÃ¡rias.
- Para deploy, utilize o `Dockerfile` ou o script `deploy.ps1` conforme sua necessidade.

---

Para dÃºvidas ou sugestÃµes, entre em contato com o mantenedor.


# Frontend

The **frontend** of the application was built with **React + Vite**.  
It allows the user to select a **JSON file** containing rÃ©sumÃ© and job description data, and send this file to the backend through the `/questions` endpoint.  
The API responds with a list of **customized questions** based on the provided rÃ©sumÃ© and job description.

---

## ğŸš€ Deployment
The frontend is hosted on a **Google Cloud Storage bucket** and can be accessed at:

ğŸ‘‰ [https://storage.googleapis.com/datathon-fiap/index.html](https://storage.googleapis.com/datathon-fiap/index.html)

---


## âš™ï¸ Running Locally

### 1. Clone the repository

You will need to clone the repo: [Datathon-Fiap](https://github.com/NathanNNB/DataThon-Fiap)
```
cd frontend
```
### 2. Install dependencies
```bash
yarn install
# or
npm install
```

### 3. Start in development mode
```bash
yarn dev
# or
npm run dev
```

### 4. Configure the environment variables

Create a `.env` file in the root of the `frontend` folder (if it doesn't exist) and add the following variable:

```env
VITE_FLASK_API_URL=http://127.0.0.1:5000
```